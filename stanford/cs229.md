# [CS229](https://alpha.athena.ai/cs229): Machine Learning

- **Code**: /stanford/cs229
- **Author**: Andrew Ng
- **Instructor**: Andrew Ng
- **Institution**: Stanford University
- **Course Website**: [http://cs229.stanford.edu](http://cs229.stanford.edu)


About this document: this is an
[Athena Linked Syllabus](https://athena.ai/about/linked-syllabus).
It is an annotated outline of a body of knowledge.
Here, this one outlines a university course, including class lecture notes,
handouts, problem sets, and topics.
Any concepts mentioned here are linked to their respective page in the
[Athena Knowledge Graph](https://athena.ai/about/knowledge-graph), where
you can find explanations, exercises, and
[concept dependencies](https://athena.ai/about/concept-dependencies).
The concept links will also
[show your mastery](https://athena.ai/about/concept-mastery).
Read more [here](https://athena.ai/about/linked-syllabus).

All content linked here is the property of its respective author(s).

### Table of Contents

[tableofcontents]


## Prerequisites


### [cs109](https://alpha.athena.ai/stanford/cs109) or [stats116](https://alpha.athena.ai/stanford/stats116)


### [cs106b](https://alpha.athena.ai/stanford/cs106b) or [cs106x](https://alpha.athena.ai/stanford/cs106x)


### [math51](https://alpha.athena.ai/stanford/math51) or [math103](https://alpha.athena.ai/stanford/math103) or [math113](https://alpha.athena.ai/stanford/math113) or [cs205](https://alpha.athena.ai/stanford/cs205)


## Outline

- Introduction (1 class)
  1. Basic concepts.

- [Supervised learning](https://alpha.athena.ai/supervised-learning) (7 classes)
  1. Supervised learning setup.
     [LMS](https://alpha.athena.ai/lms).
  1. [Logistic regression](https://alpha.athena.ai/logistic-regression).
     [Perceptron](https://alpha.athena.ai/perceptron).
     [Exponential family](https://alpha.athena.ai/exponential-family).
  1. [Generative learning algorithms](https://alpha.athena.ai/generative-learning-algorithms).
     [Gaussian discriminant analysis](https://alpha.athena.ai/gaussian-discriminant-analysis).
     [Naive Bayes](https://alpha.athena.ai/naive-bayes).
  1. [Support vector machines](https://alpha.athena.ai/support-vector-machines).
  1. [Model selection](https://alpha.athena.ai/model-selection) and [feature selection](https://alpha.athena.ai/feature-selection).
  1. [Ensemble methods](https://alpha.athena.ai/ensemble-methods): [Bagging](https://alpha.athena.ai/bagging), [boosting](https://alpha.athena.ai/boosting).
  1. Evaluating and debugging learning algorithms.

- [Learning theory](https://alpha.athena.ai/learning-theory). (3 classes)
  1. [Bias/variance tradeoff](https://alpha.athena.ai/bias/variance-tradeoff).
     Union and
     [Chernoff](https://alpha.athena.ai/chernoff)/[Hoeffding](https://alpha.athena.ai/hoeffding) bounds.
  1. [VC dimension](https://alpha.athena.ai/vc-dimension). Worst case (online) learning.
  1. Practical advice on how to use learning algorithms.

- [Unsupervised learning](https://alpha.athena.ai/unsupervised-learning). (5 classes)
  1. [Clustering](https://alpha.athena.ai/clustering).
     [K-means](https://alpha.athena.ai/k-means).
  1. [EM](https://alpha.athena.ai/em).
     [Mixture of Gaussians](https://alpha.athena.ai/mixture-of-gaussians).
  1. [Factor analysis](https://alpha.athena.ai/factor-analysis).
  1. PCA ([Principal components analysis](https://alpha.athena.ai/principal-components-analysis)).
  1. ICA ([Independent components analysis](https://alpha.athena.ai/independent-components-analysis)).

- [Reinforcement learning](https://alpha.athena.ai/reinforcement-learning) and control. (4 classes)
  1. [MDPs](https://alpha.athena.ai/mdps). [Bellman equations](https://alpha.athena.ai/bellman-equations).
  1. [Value iteration](https://alpha.athena.ai/value-iteration) and [policy iteration](https://alpha.athena.ai/policy-iteration).
  1. [Linear quadratic regulation](https://alpha.athena.ai/linear-quadratic-regulation) (LQR). LQG.
  1. [Q-learning](https://alpha.athena.ai/q-learning). [Value function approximation](https://alpha.athena.ai/value-function-approximation).
  1. [Policy search](https://alpha.athena.ai/policy-search). [Reinforce](https://alpha.athena.ai/reinforce). [POMDPs](https://alpha.athena.ai/pomdps).


## Handouts and Problem Sets

- [Course Schedule](http://cs229.stanford.edu/schedule.html)
- [Handout #1: Course Information (HTML)](http://cs229.stanford.edu/info.html)
- [Handout #2: Course Schedule (HTML)](http://cs229.stanford.edu/schedule.html)
- [Handout #3: Final Project Guidelines (PDF)new](http://cs229.stanford.edu/materials/projectGuidelines.pdf)
- [Problem Set 1 (pdf)](http://cs229.stanford.edu/materials/ps1.pdf)
  Data: [q1x.dat](http://cs229.stanford.edu/ps/ps1/q1x.dat),
  [q1y.dat](http://cs229.stanford.edu/ps/ps1/q1y.dat),
  [q2x.dat](http://cs229.stanford.edu/ps/ps1/q2x.dat),
  [q2y.dat](http://cs229.stanford.edu/ps/ps1/q2y.dat)
- [Problem Set 2 (pdf)](http://cs229.stanford.edu/materials/ps2.pdf)
  Data: [ps2.zip](http://cs229.stanford.edu/ps/ps2/ps2.zip)
- [Problem Set 3 (pdf)](http://cs229.stanford.edu/materials/ps3.pdf)
  Data: [mandrill-small.tiff](http://cs229.stanford.edu/materials/mandrill-small.tiff),
  [mandrill-large.tiff](http://cs229.stanford.edu/materials/mandrill-large.tiff)
- [Problem Set 4 (pdf)](http://cs229.stanford.edu/materials/ps4.pdf)
  Data: [q4.zip](http://cs229.stanford.edu/ps/ps4/q4.zip),
  [q6.zip](http://cs229.stanford.edu/ps/ps4/q6.zip)
- [Practice Midterm (pdf)](http://cs229.stanford.edu/materials/practice-midterm.pdf)


## Lecture Notes


### Lec. Notes 1: Supervised Learning and Discriminative Algorithms

Notes:
[ps](http://cs229.stanford.edu/notes/cs229-notes1.ps),
[pdf](http://cs229.stanford.edu/notes/cs229-notes1.pdf)

Concepts Taught:


- Part I: [Supervised Learning](https://alpha.athena.ai/supervised-learning)
  - Terminology
    - [Dataset](https://alpha.athena.ai/dataset)
    - [Input Features](https://alpha.athena.ai/input-features)
    - [Target Variable](https://alpha.athena.ai/target-variable)
    - [Training Example](https://alpha.athena.ai/training-example)
    - [Training Set](https://alpha.athena.ai/training-set)
    - [Predictor](https://alpha.athena.ai/predictor)
    - [Hypothesis Function](https://alpha.athena.ai/hypothesis-function)
    - [Learning Algorithm](https://alpha.athena.ai/learning-algorithm)
    - [Learning Problem](https://alpha.athena.ai/learning-problem)
    - [Regression problem](https://alpha.athena.ai/regression-problem)
    - [Classification problem](https://alpha.athena.ai/classification-problem)
  - [Linear Regression](https://alpha.athena.ai/linear-regression)
    - [Parameters](https://alpha.athena.ai/parameters)
    - [Intercept Term](https://alpha.athena.ai/intercept-term)
    - [Cost Function](https://alpha.athena.ai/cost-function)
    - [Least-Squares Cost Function](https://alpha.athena.ai/least-squares-cost-function)
    - [Ordinary Least Squares](https://alpha.athena.ai/ordinary-least-squares)
    - [Least Mean Squares Algorithm](https://alpha.athena.ai/least-mean-squares-algorithm)
      - [Gradient Descent Algorithm](https://alpha.athena.ai/gradient-descent-algorithm)
      - [Learning Rate](https://alpha.athena.ai/learning-rate)
      - [Update Rule](https://alpha.athena.ai/update-rule)
      - [Widrow-Hoﬀ Learning Rule](https://alpha.athena.ai/widrow-hoﬀ-learning-rule)
      - [Error Term](https://alpha.athena.ai/error-term)
      - [Batch Gradient Descent](https://alpha.athena.ai/batch-gradient-descent)
      - [Convex Function](https://alpha.athena.ai/convex-function)
      - [Stochastic Gradient Descent](https://alpha.athena.ai/stochastic-gradient-descent)
    - [Normal Equations](https://alpha.athena.ai/normal-equations)
      - [Matrix Derivatives](https://alpha.athena.ai/matrix-derivatives)
      - [trace operator](https://alpha.athena.ai/trace-operator)
      - [Design Matrix](https://alpha.athena.ai/design-matrix)
    - Probabilistic Interpretation
      - [Likelihood Function](https://alpha.athena.ai/likelihood-function)
      - [Maximum Likelihood Principle](https://alpha.athena.ai/maximum-likelihood-principle)
      - [Log Likelihood Function](https://alpha.athena.ai/log-likelihood-function)
    - [Locally Weighted Linear Regression](https://alpha.athena.ai/locally-weighted-linear-regression)
      - [Underfitting](https://alpha.athena.ai/underfitting)
      - [Overfitting](https://alpha.athena.ai/overfitting)
      - [Bandwidth Parameter](https://alpha.athena.ai/bandwidth-parameter)
      - [Non-Parametric Learning Algorithm](https://alpha.athena.ai/non-parametric-learning-algorithm)
      - [Parametric Learning Algorithm](https://alpha.athena.ai/parametric-learning-algorithm)
- Part II:
  [Classification](https://alpha.athena.ai/classification) and
  [Logistic Regression](https://alpha.athena.ai/logistic-regression)
  - Terminology
    - [Binary Classification](https://alpha.athena.ai/binary-classification)
    - [Training Example Label](https://alpha.athena.ai/training-example-label)
  - [Logistic Regression](https://alpha.athena.ai/logistic-regression)
    - [Logistic Function](https://alpha.athena.ai/logistic-function)
    - [Maximum Likelihood with Logistic Regression](https://alpha.athena.ai/maximum-likelihood-with-logistic-regression)
    - [Logistic Regression Gradient Ascent](https://alpha.athena.ai/logistic-regression-gradient-ascent)
  - [Perceptron Learning Algorithm](https://alpha.athena.ai/perceptron-learning-algorithm)
  - Maximizing with Newton's Method
    - [Newton-Raphson Method](https://alpha.athena.ai/newton-raphson-method)
    - [Hessian Matrix](https://alpha.athena.ai/hessian-matrix)
    - [Fisher Scoring](https://alpha.athena.ai/fisher-scoring)
- Part III: [Generalized Linear Models](https://alpha.athena.ai/generalized-linear-models)
  - [Exponential Family Distributions](https://alpha.athena.ai/exponential-family-distributions)
    - [Natural Parameter](https://alpha.athena.ai/natural-parameter) or [Canonical Parameter](https://alpha.athena.ai/canonical-parameter)
    - [Sufficient Statistic](https://alpha.athena.ai/sufficient-statistic)
    - [Log Partition Function](https://alpha.athena.ai/log-partition-function)
    - [Distribution Family](https://alpha.athena.ai/distribution-family)
    - [Bernouilli as Exponential Family Distribution](https://alpha.athena.ai/bernouilli-as-exponential-family-distribution)
    - [Normal as Exponential Family Distribution](https://alpha.athena.ai/normal-as-exponential-family-distribution)
    - [Dispersion Parameter](https://alpha.athena.ai/dispersion-parameter)
  - [Constructing Generalized Linear Models](https://alpha.athena.ai/constructing-generalized-linear-models)
    - [Ordinary Least Squares as Generalized Linear Model](https://alpha.athena.ai/ordinary-least-squares-as-generalized-linear-model)
    - [Logistic Regression as Generalized Linear Model](https://alpha.athena.ai/logistic-regression-as-generalized-linear-model)
      - [Canonical Response Function](https://alpha.athena.ai/canonical-response-function)
      - [Canonical Link Function](https://alpha.athena.ai/canonical-link-function)
    - [Softmax Regression as Generalized Linear Model](https://alpha.athena.ai/softmax-regression-as-generalized-linear-model)
      - [Multinomial as Exponential Family Distribution](https://alpha.athena.ai/multinomial-as-exponential-family-distribution)
      - [Softmax Regression](https://alpha.athena.ai/softmax-regression)


Concepts Required

- [Linear Algebra](https://alpha.athena.ai/linear-algebra)
  - [Matrix Arithmetic](https://alpha.athena.ai/matrix-arithmetic)

- [Probability Theory](https://alpha.athena.ai/probability-theory)
  - [Normal Distribution](https://alpha.athena.ai/normal-distribution)
  - [IID RVs](https://alpha.athena.ai/iid-rvs)
  - [Maximum Likelihood Estimate](https://alpha.athena.ai/maximum-likelihood-estimate)



### Lec. Notes 2: Generative Algorithms

Notes:
[ps](http://cs229.stanford.edu/notes/cs229-notes2.ps),
[pdf](http://cs229.stanford.edu/notes/cs229-notes2.pdf)

Concepts Taught:


### Lec. Notes 3: Support Vector Machines

Notes:
[ps](http://cs229.stanford.edu/notes/cs229-notes3.ps),
[pdf](http://cs229.stanford.edu/notes/cs229-notes3.pdf)

Concepts Taught:


### Lec. Notes 4: Learning Theory

Notes:
[ps](http://cs229.stanford.edu/notes/cs229-notes4.ps),
[pdf](http://cs229.stanford.edu/notes/cs229-notes4.pdf)

Concepts Taught:


### Lec. Notes 5: Regularization and Model Selection

Notes:
[ps](http://cs229.stanford.edu/notes/cs229-notes5.ps),
[pdf](http://cs229.stanford.edu/notes/cs229-notes5.pdf)

Concepts Taught:


### Lec. Notes 6: Online Learning and the Perceptron Algorithm. (optional reading)

Notes:
[ps](http://cs229.stanford.edu/notes/cs229-notes6.ps),
[pdf](http://cs229.stanford.edu/notes/cs229-notes6.pdf)

Concepts Taught:


### Lec. Notes 7a: Unsupervised Learning, k-means clustering.

Notes:
[ps](http://cs229.stanford.edu/notes/cs229-notes7a.ps),
[pdf](http://cs229.stanford.edu/notes/cs229-notes7a.pdf)

Concepts Taught:


### Lec. Notes 7b: Mixture of Gaussians

Notes:
[ps](http://cs229.stanford.edu/notes/cs229-notes7b.ps),
[pdf](http://cs229.stanford.edu/notes/cs229-notes7b.pdf)

Concepts Taught:


### Lec. Notes 8: The EM Algorithm

Notes:
[ps](http://cs229.stanford.edu/notes/cs229-notes8.ps),
[pdf](http://cs229.stanford.edu/notes/cs229-notes8.pdf)

Concepts Taught:


### Lec. Notes 9: Factor Analysis

Notes:
[ps](http://cs229.stanford.edu/notes/cs229-notes9.ps),
[pdf](http://cs229.stanford.edu/notes/cs229-notes9.pdf)

Concepts Taught:


### Lec. Notes 10: Principal Components Analysis

Notes:
[ps](http://cs229.stanford.edu/notes/cs229-notes10.ps),
[pdf](http://cs229.stanford.edu/notes/cs229-notes10.pdf)

Concepts Taught:


### Lec. Notes 11: Independent Components Analysis

Notes:
[ps](http://cs229.stanford.edu/notes/cs229-notes11.ps),
[pdf](http://cs229.stanford.edu/notes/cs229-notes11.pdf)

Concepts Taught:


### Lec. Notes 12: Reinforcement Learning and Control


Notes:
[ps](http://cs229.stanford.edu/notes/cs229-notes12.ps),
[pdf](http://cs229.stanford.edu/notes/cs229-notes12.pdf)

Concepts Taught:



## Section Notes


### Section notes 1:  Linear Algebra Review and Reference

Notes: [pdf](http://cs229.stanford.edu/section/cs229-linalg.pdf)

Concepts Taught:


### Section notes 2:  Probability Theory Review

Notes: [pdf](http://cs229.stanford.edu/section/cs229-prob.pdf)

Concepts Taught:


### Matlab tutorial

Files:
[sigmoid.m](http://cs229.stanford.edu/section/matlab/sigmoid.m),
[logistic_grad_ascent.m](http://cs229.stanford.edu/section/matlab/logistic_grad_ascent.m),
[matlab_session.m](http://cs229.stanford.edu/section/matlab/matlab_session.m)

Concepts Taught:


### Section notes 4:  Convex Optimization Overview, Part I

Notes:
[ps](http://cs229.stanford.edu/section/cs229-cvxopt.ps),
[pdf](http://cs229.stanford.edu/section/cs229-cvxopt.pdf)

Concepts Taught:


### Section notes 5:  Convex Optimization Overview, Part II

Notes:
[ps](http://cs229.stanford.edu/section/cs229-cvxopt2.ps),
[pdf](http://cs229.stanford.edu/section/cs229-cvxopt2.pdf)

Concepts Taught:


### Section notes 6:  Hidden Markov Models

Notes:
[ps](http://cs229.stanford.edu/section/cs229-hmm.ps),
[pdf](http://cs229.stanford.edu/section/cs229-hmm.pdf)

Concepts Taught:


### Section notes 7:  The Multivariate Gaussian Distribution

Notes: [pdf](http://cs229.stanford.edu/section/gaussians.pdf)

Concepts Taught:

### Section notes 8:  More on Gaussian Distribution

Notes: [pdf](http://cs229.stanford.edu/section/more_on_gaussians.pdf)

Concepts Taught:

### Section notes 9:  Gaussian Processes

Notes: [pdf](http://cs229.stanford.edu/section/cs229-gaussian_processes.pdf)

Concepts Taught:


##  Other resources

Advice on applying machine learning: Slides from Andrew's lecture on getting
machine learning algorithms to work in practice can be found
[here](http://cs229.stanford.edu/materials/ML-advice.pdf).

Previous projects: A list of last year's final projects can be found
[here](http://cs229.stanford.edu/projects2011.html).


### Matlab, Octave

Matlab resources: Here are a couple of Matlab tutorials that you might find
helpful: http://www.math.ufl.edu/help/matlab-tutorial/ and
http://www.math.mtu.edu/~msgocken/intro/node1.html. For emacs users only: If
you plan to run Matlab in emacs, here are
[matlab.el](http://cs229.stanford.edu/materials/matlab.el), and a helpful
[.emacs](http://cs229.stanford.edu/materials/emacs) file.

Octave resources: For a free alternative to Matlab, check out
[GNU Octave](http://www.gnu.org/software/octave/). The official documentation
is available [here](http://www.gnu.org/software/octave/doc/interpreter/). Some
useful tutorials on Octave include
http://en.wikibooks.org/wiki/Octave_Programming_Tutorial and
http://www-mdp.eng.cam.ac.uk/web/CD/engapps/octave/octavetut.pdf .

### Data
Here is the [UCI Machine learning repository](http://www.ics.uci.edu/~mlearn/MLRepository.html), which contains a
large collection of standard datasets for testing learning algorithms. If you
want to see examples of recent work in machine learning, start by taking a look
at the conferences [NIPS](http://www.nips.cc/) (all old NIPS papers are online)
and ICML. Some other related conferences include UAI, AAAI, IJCAI.

### Viewing PostScript and PDF files

Depending on the computer you are using, you
may be able to download a [PostScript viewer](http://www.cs.wisc.edu/~ghost/)
or [PDF viewer](http://get.adobe.com/reader/otherversions/) for it if you don't
already have one.
